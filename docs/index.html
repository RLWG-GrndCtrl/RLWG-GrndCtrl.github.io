<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/images/grndctrl_pipeline.png"/><link rel="stylesheet" href="/_next/static/css/5e2874213eabdbe4.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-464b3adafe9e4e9b.js"/><script src="/_next/static/chunks/a8bb8ad9-fc2ec1eca04179b0.js" async=""></script><script src="/_next/static/chunks/525-4bf0f37a1bc3cf60.js" async=""></script><script src="/_next/static/chunks/main-app-ba5ef907abe65c3b.js" async=""></script><script src="/_next/static/chunks/976-8ed120f2ef864363.js" async=""></script><script src="/_next/static/chunks/20-50608d3ee5103ab2.js" async=""></script><script src="/_next/static/chunks/app/page-916c3d4f2cfcbb12.js" async=""></script><script src="/_next/static/chunks/ce84277d-606566995e0e358b.js" async=""></script><script src="/_next/static/chunks/app/error-45794d858c12518b.js" async=""></script><title>GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment</title><meta name="description" content="Reinforcement Learning with World Grounding (RLWG) addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards."/><link rel="manifest" href="/favicon/site.webmanifest" crossorigin="use-credentials"/><meta name="robots" content="index, follow"/><meta property="og:title" content="GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment"/><meta property="og:description" content="Reinforcement Learning with World Grounding (RLWG) addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards."/><meta property="og:url" content="https://rlwg-grndctrl.github.io"/><meta property="og:site_name" content="GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment"/><meta property="og:locale" content="en_US"/><meta property="og:image" content="https://rlwg-grndctrl.github.io/images/og.jpg"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment"/><meta name="twitter:description" content="Reinforcement Learning with World Grounding (RLWG) addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards."/><meta name="twitter:image" content="https://rlwg-grndctrl.github.io/images/og.jpg"/><link rel="shortcut icon" href="/favicon/favicon-16x16.png"/><link rel="icon" href="/favicon/favicon.ico"/><link rel="apple-touch-icon" href="/favicon/apple-touch-icon.png"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><main><section class="bg-white text-gray-600 relative flex items-center justify-center h-screen overflow-hidden"><div class="layout z-20 relative flex min-h-screen flex-col items-center justify-center p-4 text-center"><h1 class="mt-4 text-5xl mb-4"><img src="/images/icon.png" alt="GrndCtrl icon" class="h-12 inline-block mr-3 align-middle" loading="lazy"/>GrndCtrl: <!-- --> <span class="text-primary-600">Gr</span>ounding <!-- --> <span class="text-primary-600">W</span>orld <!-- --> <span class="text-primary-600">M</span>odels via <!-- --> <span class="text-primary-600">S</span>elf-Supervised <!-- --> <span class="text-primary-600">R</span>eward <!-- --> <span class="text-primary-600">A</span>lignment</h1><div class="container pb-6"><span class="text-lg"><span class="text-gray-500">Authors: [To be updated]</span></span></div><div class="container flex flex-row items-center space-x-8 justify-center text-lg"><a class="animated-underline custom-link inline-flex items-center font-medium focus-visible:ring-primary-500 focus:outline-none focus-visible:rounded focus-visible:ring focus-visible:ring-offset-2 border-dark border-b border-dotted hover:border-black/0 group gap-[0.25em] mt-6" variant="light" href="#"><img src="/svg/arxiv.svg" alt="arXiv logo" class="h-6 w-6 shrink-0" loading="lazy"/><span>arXiv Page</span><svg viewBox="0 0 16 16" height="1em" width="1em" fill="none" xmlns="http://www.w3.org/2000/svg" class="relative transition-transform duration-200 motion-safe:-translate-x-1 group-hover:translate-x-0"><path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path><path stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round" class="origin-left transition-all duration-200 opacity-0 motion-safe:-translate-x-1 group-hover:translate-x-0 group-hover:opacity-100"></path></svg></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/RLWG-GrndCtrl" variant="light" class="cursor-newtab animated-underline custom-link inline-flex items-center font-medium focus-visible:ring-primary-500 focus:outline-none focus-visible:rounded focus-visible:ring focus-visible:ring-offset-2 border-dark border-b border-dotted hover:border-black/0 group gap-[0.25em] mt-6"><img src="/svg/github.svg" alt="GitHub logo" class="h-6 w-6 shrink-0" loading="lazy"/><span>GitHub Repo</span><svg viewBox="0 0 16 16" height="1em" width="1em" fill="none" xmlns="http://www.w3.org/2000/svg" class="relative transition-transform duration-200 motion-safe:-translate-x-1 group-hover:translate-x-0"><path fill="currentColor" d="M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"></path><path stroke="currentColor" d="M1.75 8H11" stroke-width="1.5" stroke-linecap="round" class="origin-left transition-all duration-200 opacity-0 motion-safe:-translate-x-1 group-hover:translate-x-0 group-hover:opacity-100"></path></svg></a></div></div><div class="absolute w-auto min-w-full min-h-full max-w-none z-10 bg-stone-300/70"></div><div class="absolute inset-0 flex items-center justify-center z-0 opacity-10"><img src="/images/icon.png" alt="Background icon" class="w-1/3 h-1/3 object-contain" loading="lazy"/></div></section><section class="bg-white text-gray-600"><div class="layout py-12"><h2 class="text-center pb-4">Abstract</h2><p class="text-pretty">Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce<b> Reinforcement Learning with World Grounding (RLWG)</b>, a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with <b>GrndCtrl</b>, a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, <b>GrndCtrl</b> leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments.</p></div></section><section class="bg-dark text-gray-200 py-8"><div class="relative"><div class="flex overflow-x-auto snap-x snap-mandatory scroll-smooth gap-6 px-48 pb-2 scrollbar-dark" aria-label="GrndCtrl highlights slider"><article class="flex-none snap-center w-[85%] md:w-[70%] lg:w-[55%] bg-gray-900/50 border border-white/10 rounded-2xl p-6 shadow-2xl backdrop-blur"><h3 class="text-2xl font-semibold mb-4">Overview</h3><div class="space-y-4 text-base leading-relaxed"><p><b>Reinforcement Learning with World Grounding (RLWG)</b> addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards. Instead of reconstruction losses, RLWG grounds models using geometric and perceptual rewards from frozen evaluators.</p><p><b>GrndCtrl</b> instantiates RLWG using Group Relative Policy Optimization (GRPO), enabling physically consistent rollouts essential for reliable world generation.</p></div></article><article class="flex-none snap-center w-[85%] md:w-[70%] lg:w-[55%] bg-gray-900/50 border border-white/10 rounded-2xl p-6 shadow-2xl backdrop-blur"><h3 class="text-2xl font-semibold mb-4">Problem</h3><div class="space-y-4 text-base leading-relaxed"><p>Despite impressive generative fidelity, current video world models often capture the<em> appearance</em> of motion more than its <em>structure</em>. Their rollouts remain visually plausible but geometrically and temporally inconsistent: poses drift, depths wobble, and trajectories lose alignment over time.</p><p>These instabilities limit the use of current models for closed-loop tasks such as localization, mapping, and planning, where physically consistent representation is essential.</p></div></article><article class="flex-none snap-center w-[85%] md:w-[70%] lg:w-[55%] bg-gray-900/50 border border-white/10 rounded-2xl p-6 shadow-2xl backdrop-blur"><h3 class="text-2xl font-semibold mb-4">Solution</h3><div class="space-y-4 text-base leading-relaxed"><p><b>RLWG</b> refines pretrained world models using verifiable geometric and perceptual rewards derived from model rollouts. Each rollout is automatically scored using rewards that quantify spatial and temporal coherence, such as pose cycle-consistency, depth reprojection agreement, and action adherence.</p><p><b>GrndCtrl</b> uses GRPO to optimize these verifiable rewards efficiently, preserving visual quality while progressively aligning the model&#x27;s dynamics with measurable structure in the real world.</p></div></article></div><div class="pointer-events-none absolute inset-y-0 left-0 w-10 bg-gradient-to-r from-dark to-transparent"></div><div class="pointer-events-none absolute inset-y-0 right-0 w-10 bg-gradient-to-l from-dark to-transparent"></div></div></section><section class="bg-white text-gray-600"><div class="layout py-12"><h2 class="pb-4">Method</h2><figure class="flex flex-col items-center justify-center"><img src="/images/grndctrl_pipeline.png" class="w-full h-auto rounded-md pt-4 pb-4 transition invert-0"/><figcaption class="text-gray-600 mt-2 font-light">Figure <!-- -->1<!-- -->. <!-- -->Overview of GrndCtrl. RLWG refines pretrained world models using verifiable geometric and perceptual rewards. GrndCtrl instantiates RLWG using Group Relative Policy Optimization (GRPO) to optimize these rewards, enabling physically consistent rollouts.</figcaption></figure></div></section><section class="bg-gray-100 text-gray-600"><div class="layout pt-4 pb-48"><h2 class="mt-12 mb-4">Citation</h2><pre class="ml-12">@article{grndctrl2026,
      title={GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment}, 
      author={[Authors to be added]},
      journal={CVPR},
      year={2026},
      url={https://arxiv.org/abs/[arxiv-id]}, 
}</pre></div></section></main><script src="/_next/static/chunks/webpack-464b3adafe9e4e9b.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/5e2874213eabdbe4.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2054,[],\"\"]\n4:I[7784,[],\"ClientPageRoot\"]\n5:I[2733,[\"976\",\"static/chunks/976-8ed120f2ef864363.js\",\"20\",\"static/chunks/20-50608d3ee5103ab2.js\",\"931\",\"static/chunks/app/page-916c3d4f2cfcbb12.js\"],\"default\",1]\n6:I[1947,[],\"\"]\n7:I[2354,[\"648\",\"static/chunks/ce84277d-606566995e0e358b.js\",\"976\",\"static/chunks/976-8ed120f2ef864363.js\",\"601\",\"static/chunks/app/error-45794d858c12518b.js\"],\"default\"]\n8:I[8092,[],\"\"]\na:I[2805,[],\"\"]\nb:[]\n0:[\"$\",\"$L2\",null,{\"buildId\":\"Qnh8jUv-62vFaopXoTyed\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"\"],\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L3\",[\"$\",\"$L4\",null,{\"props\":{\"params\":{},\"searchParams\":{}},\"Component\":\"$5\"}],null],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5e2874213eabdbe4.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"children\":[\"$\",\"body\",null,{\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$7\",\"errorStyles\":[],\"errorScripts\":[],\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"main\",null,{\"children\":[\"$\",\"section\",null,{\"className\":\"bg-white\",\"children\":[\"$\",\"div\",null,{\"className\":\"layout flex min-h-screen flex-col items-center justify-center text-center text-black\",\"children\":[[\"$\",\"svg\",null,{\"stroke\":\"currentColor\",\"fill\":\"currentColor\",\"strokeWidth\":\"0\",\"viewBox\":\"0 0 24 24\",\"className\":\"drop-shadow-glow animate-flicker text-red-500\",\"children\":[\"$undefined\",[[\"$\",\"path\",\"0\",{\"d\":\"M4.00098 20V14C4.00098 9.58172 7.5827 6 12.001 6C16.4193 6 20.001 9.58172 20.001 14V20H21.001V22H3.00098V20H4.00098ZM6.00098 14H8.00098C8.00098 11.7909 9.79184 10 12.001 10V8C8.68727 8 6.00098 10.6863 6.00098 14ZM11.001 2H13.001V5H11.001V2ZM19.7792 4.80761L21.1934 6.22183L19.0721 8.34315L17.6578 6.92893L19.7792 4.80761ZM2.80859 6.22183L4.22281 4.80761L6.34413 6.92893L4.92991 8.34315L2.80859 6.22183Z\",\"children\":\"$undefined\"}]]],\"style\":{\"color\":"])</script><script>self.__next_f.push([1,"\"$undefined\"},\"height\":60,\"width\":60,\"xmlns\":\"http://www.w3.org/2000/svg\"}],[\"$\",\"h1\",null,{\"className\":\"mt-8 text-4xl md:text-6xl\",\"children\":\"Page Not Found\"}],[\"$\",\"a\",null,{\"href\":\"/\",\"children\":\"Back to home\"}]]}]}]}],\"notFoundStyles\":[]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L9\"],\"globalErrorComponent\":\"$a\",\"missingSlots\":\"$Wb\"}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Reinforcement Learning with World Grounding (RLWG) addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards.\"}],[\"$\",\"link\",\"4\",{\"rel\":\"manifest\",\"href\":\"/favicon/site.webmanifest\",\"crossOrigin\":\"use-credentials\"}],[\"$\",\"meta\",\"5\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Reinforcement Learning with World Grounding (RLWG) addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://rlwg-grndctrl.github.io\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image\",\"content\":\"https://rlwg-grndctrl.github.io/images/og.jpg\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:title\",\"content\":\"GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:description\",\"content\":\"Reinforcement Learning with World Grounding (RLWG) addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards.\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:image\",\"content\":\"https://rlwg-grndctrl.github.io/images/og.jpg\"}],[\"$\",\"link\",\"17\",{\"rel\":\"shortcut icon\",\"href\":\"/favicon/favicon-16x16.png\"}],[\"$\",\"link\",\"18\",{\"rel\":\"icon\",\"href\":\"/favicon/favicon.ico\"}],[\"$\",\"link\",\"19\",{\"rel\":\"apple-touch-icon\",\"href\":\"/favicon/apple-touch-icon.png\"}]]\n"])</script><script>self.__next_f.push([1,"3:null\n"])</script></body></html>