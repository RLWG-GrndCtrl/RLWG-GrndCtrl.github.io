(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[931],{3002:function(e,r,t){Promise.resolve().then(t.bind(t,2733))},2733:function(e,r,t){"use strict";t.r(r),t.d(r,{default:function(){return x}});var s=t(3735),i=t(7381),n=t(294),a=t(3092),l=t(3654);a.Ry({NEXT_PUBLIC_SHOW_LOGGER:a.Km(["true","false"]).optional()}).parse(l.env);var o=t(5797),c=function(e){let{state:r,switch_state:t,size:n="normal"}=e;return(0,s.jsx)(o.rs,{checked:r,onChange:t,className:(0,i.Z)("group inline-flex  items-center rounded-full bg-gray-600 transition data-[checked]:bg-primary-500","normal"===n?"h-6 w-11":"h-4 w-9"),children:(0,s.jsx)("span",{className:(0,i.Z)("normal"===n?"size-4 group-data-[checked]:translate-x-6":"size-3 group-data-[checked]:translate-x-5","translate-x-1 rounded-full bg-white transition")})})},d=e=>{let{img_src:r,caption:t,isDark:a,idx:l}=e,o=a?"text-gray-400":"text-gray-600",[d,m]=(0,n.useState)(a?"invert":"invert-0");return(0,n.useEffect)(()=>{m(a?"invert":"invert-0")},[a]),(0,s.jsxs)("figure",{className:"flex flex-col items-center justify-center",children:[(0,s.jsx)("img",{src:r,className:(0,i.Z)("w-full h-auto rounded-md pt-4 pb-4 transition",d,"invert"===d?"bg-gray-200":"")}),a?(0,s.jsxs)("div",{className:(0,i.Z)(o,"text-sm","invert"===d?"text-primary-500":""),children:[(0,s.jsxs)("span",{children:["Color Inversion ","invert"===d?"ON":"OFF"," "]}),(0,s.jsx)(c,{state:"invert"===d,switch_state:()=>m("invert"===d?"invert-0":"invert"),size:"small"})]}):null,(0,s.jsxs)("figcaption",{className:(0,i.Z)(o,"mt-2","font-light"),children:["Figure ",l,". ",t]})]})},m=t(9459),h=t(508);let u=n.forwardRef((e,r)=>{let{children:t,href:i,openNewTab:n,className:a,nextLinkProps:l,...o}=e;return(void 0!==n?n:i&&!i.startsWith("/")&&!i.startsWith("#"))?(0,s.jsx)("a",{ref:r,target:"_blank",rel:"noopener noreferrer",href:i,...o,className:(0,m.cn)("cursor-newtab",a),children:t}):(0,s.jsx)(h.default,{href:i,ref:r,className:a,...o,...l,children:t})}),g=n.forwardRef((e,r)=>{let{children:t,className:i,...n}=e;return(0,s.jsx)(u,{ref:r,...n,className:(0,m.cn)("animated-underline custom-link inline-flex items-center font-medium","focus-visible:ring-primary-500 focus:outline-none focus-visible:rounded focus-visible:ring focus-visible:ring-offset-2","border-dark border-b border-dotted hover:border-black/0",i),children:t})});function p(e){let{children:r,className:t,direction:i="right",as:n,icon:a=null,...l}=e;return(0,s.jsxs)(n||g,{...l,className:(0,m.cn)("group gap-[0.25em]","left"===i&&"flex-row-reverse",t),children:[a,(0,s.jsx)("span",{children:r}),(0,s.jsxs)("svg",{viewBox:"0 0 16 16",height:"1em",width:"1em",fill:"none",xmlns:"http://www.w3.org/2000/svg",className:(0,m.cn)("relative","transition-transform duration-200","right"===i?"motion-safe:-translate-x-1":"rotate-180","group-hover:translate-x-0"),children:[(0,s.jsx)("path",{fill:"currentColor",d:"M7.28033 3.21967C6.98744 2.92678 6.51256 2.92678 6.21967 3.21967C5.92678 3.51256 5.92678 3.98744 6.21967 4.28033L7.28033 3.21967ZM11 8L11.5303 8.53033C11.8232 8.23744 11.8232 7.76256 11.5303 7.46967L11 8ZM6.21967 11.7197C5.92678 12.0126 5.92678 12.4874 6.21967 12.7803C6.51256 13.0732 6.98744 13.0732 7.28033 12.7803L6.21967 11.7197ZM6.21967 4.28033L10.4697 8.53033L11.5303 7.46967L7.28033 3.21967L6.21967 4.28033ZM10.4697 7.46967L6.21967 11.7197L7.28033 12.7803L11.5303 8.53033L10.4697 7.46967Z"}),(0,s.jsx)("path",{stroke:"currentColor",d:"M1.75 8H11",strokeWidth:"1.5",strokeLinecap:"round",className:(0,m.cn)("origin-left transition-all duration-200","opacity-0 motion-safe:-translate-x-1","group-hover:translate-x-0 group-hover:opacity-100")})]})]})}function x(){let e="text-gray-600",r="bg-white",t="text-primary-600",a="h-6 w-6 shrink-0",l=[{title:"Overview",content:(0,s.jsxs)("div",{className:"space-y-4 text-base leading-relaxed",children:[(0,s.jsxs)("p",{children:[(0,s.jsx)("b",{children:"Reinforcement Learning with World Grounding (RLWG)"})," addresses geometric inconsistencies in pretrained video world models through self-supervised post-training with verifiable rewards. Instead of reconstruction losses, RLWG grounds models using geometric and perceptual rewards from frozen evaluators."]}),(0,s.jsxs)("p",{children:[(0,s.jsx)("b",{children:"GrndCtrl"})," instantiates RLWG using Group Relative Policy Optimization (GRPO), enabling physically consistent rollouts essential for reliable world generation."]})]})},{title:"Problem",content:(0,s.jsxs)("div",{className:"space-y-4 text-base leading-relaxed",children:[(0,s.jsxs)("p",{children:["Despite impressive generative fidelity, current video world models often capture the",(0,s.jsx)("em",{children:" appearance"})," of motion more than its ",(0,s.jsx)("em",{children:"structure"}),". Their rollouts remain visually plausible but geometrically and temporally inconsistent: poses drift, depths wobble, and trajectories lose alignment over time."]}),(0,s.jsx)("p",{children:"These instabilities limit the use of current models for closed-loop tasks such as localization, mapping, and planning, where physically consistent representation is essential."})]})},{title:"Solution",content:(0,s.jsxs)("div",{className:"space-y-4 text-base leading-relaxed",children:[(0,s.jsxs)("p",{children:[(0,s.jsx)("b",{children:"RLWG"})," refines pretrained world models using verifiable geometric and perceptual rewards derived from model rollouts. Each rollout is automatically scored using rewards that quantify spatial and temporal coherence, such as pose cycle-consistency, depth reprojection agreement, and action adherence."]}),(0,s.jsxs)("p",{children:[(0,s.jsx)("b",{children:"GrndCtrl"})," uses GRPO to optimize these verifiable rewards efficiently, preserving visual quality while progressively aligning the model's dynamics with measurable structure in the real world."]})]})}],o=(0,n.useRef)(null);return(0,n.useEffect)(()=>{let e=o.current;if(!e)return;let r=e.children[1];if(!r)return;let t=e.clientWidth,s=r.clientWidth,i=r.offsetLeft-(t-s)/2;e.scrollLeft=i},[]),(0,s.jsxs)("main",{children:[(0,s.jsxs)("section",{className:(0,i.Z)(r,e,"relative flex items-center justify-center h-screen overflow-hidden"),children:[(0,s.jsxs)("div",{className:"layout z-20 relative flex min-h-screen flex-col items-center justify-center p-4 text-center",children:[(0,s.jsxs)("h1",{className:"mt-4 text-5xl mb-4",children:[(0,s.jsx)("img",{src:"/images/icon.png",alt:"GrndCtrl icon",className:"h-12 inline-block mr-3 align-middle",loading:"lazy"}),"GrndCtrl: "," ",(0,s.jsx)("span",{className:t,children:"Gr"}),"ounding "," ",(0,s.jsx)("span",{className:t,children:"W"}),"orld "," ",(0,s.jsx)("span",{className:t,children:"M"}),"odels via "," ",(0,s.jsx)("span",{className:t,children:"S"}),"elf-Supervised "," ",(0,s.jsx)("span",{className:t,children:"R"}),"eward "," ",(0,s.jsx)("span",{className:t,children:"A"}),"lignment"]}),(0,s.jsx)("div",{className:"container pb-6",children:(0,s.jsx)("span",{className:"text-lg",children:(0,s.jsx)("span",{className:"text-gray-500",children:"Authors: [To be updated]"})})}),(0,s.jsxs)("div",{className:"container flex flex-row items-center space-x-8 justify-center text-lg",children:[(0,s.jsx)(p,{className:"mt-6",href:"#",variant:"light",size:"large",icon:(0,s.jsx)("img",{src:"/svg/arxiv.svg",alt:"arXiv logo",className:a,loading:"lazy"}),children:"arXiv Page"}),(0,s.jsx)(p,{className:"mt-6",href:"https://github.com/RLWG-GrndCtrl",variant:"light",size:"large",icon:(0,s.jsx)("img",{src:"/svg/github.svg",alt:"GitHub logo",className:a,loading:"lazy"}),children:"GitHub Repo"})]})]}),(0,s.jsx)("div",{className:(0,i.Z)("absolute w-auto min-w-full min-h-full max-w-none z-10","bg-stone-300/70")}),(0,s.jsx)("div",{className:"absolute inset-0 flex items-center justify-center z-0 opacity-10",children:(0,s.jsx)("img",{src:"/images/icon.png",alt:"Background icon",className:"w-1/3 h-1/3 object-contain",loading:"lazy"})})]}),(0,s.jsx)("section",{className:(0,i.Z)(r,e),children:(0,s.jsxs)("div",{className:"layout py-12",children:[(0,s.jsx)("h2",{className:"text-center pb-4",children:"Abstract"}),(0,s.jsxs)("p",{className:"text-pretty",children:["Recent advances in video world modeling have enabled large-scale generative models to simulate embodied environments with high visual fidelity, providing strong priors for prediction, planning, and control. Yet, despite their realism, these models often lack geometric grounding, limiting their use in navigation tasks that require spatial coherence and long-horizon stability. We introduce",(0,s.jsx)("b",{children:" Reinforcement Learning with World Grounding (RLWG)"}),", a self-supervised post-training framework that aligns pretrained world models with a physically verifiable structure through geometric and perceptual rewards. Analogous to reinforcement learning from verifiable feedback (RLVR) in language models, RLWG can use multiple rewards that measure pose cycle-consistency, depth reprojection, and temporal coherence. We instantiate this framework with ",(0,s.jsx)("b",{children:"GrndCtrl"}),", a reward-aligned adaptation method based on Group Relative Policy Optimization (GRPO), yielding world models that maintain stable trajectories, consistent geometry, and reliable rollouts for embodied navigation. Like post-training alignment in large language models, ",(0,s.jsx)("b",{children:"GrndCtrl"})," leverages verifiable rewards to bridge generative pretraining and grounded behavior, achieving superior spatial coherence and navigation stability over supervised fine-tuning in outdoor environments."]})]})}),(0,s.jsx)("section",{className:"bg-dark text-gray-200 py-8",children:(0,s.jsxs)("div",{className:"relative",children:[(0,s.jsx)("div",{ref:o,className:"flex overflow-x-auto snap-x snap-mandatory scroll-smooth gap-6 px-48 pb-2 scrollbar-dark","aria-label":"GrndCtrl highlights slider",children:l.map(e=>(0,s.jsxs)("article",{className:"flex-none snap-center w-[85%] md:w-[70%] lg:w-[55%] bg-gray-900/50 border border-white/10 rounded-2xl p-6 shadow-2xl backdrop-blur",children:[(0,s.jsx)("h3",{className:"text-2xl font-semibold mb-4",children:e.title}),e.content]},e.title))}),(0,s.jsx)("div",{className:"pointer-events-none absolute inset-y-0 left-0 w-10 bg-gradient-to-r from-dark to-transparent"}),(0,s.jsx)("div",{className:"pointer-events-none absolute inset-y-0 right-0 w-10 bg-gradient-to-l from-dark to-transparent"})]})}),(0,s.jsx)("section",{className:(0,i.Z)(r,e),children:(0,s.jsxs)("div",{className:"layout py-12",children:[(0,s.jsx)("h2",{className:"pb-4",children:"Method"}),(0,s.jsx)(d,{img_src:"/images/grndctrl_pipeline.png",caption:"Overview of GrndCtrl. RLWG refines pretrained world models using verifiable geometric and perceptual rewards. GrndCtrl instantiates RLWG using Group Relative Policy Optimization (GRPO) to optimize these rewards, enabling physically consistent rollouts.",isDark:!1,idx:1})]})}),(0,s.jsx)("section",{className:(0,i.Z)("bg-gray-100",e),children:(0,s.jsxs)("div",{className:"layout pt-4 pb-48",children:[(0,s.jsx)("h2",{className:"mt-12 mb-4",children:"Citation"}),(0,s.jsx)("pre",{className:"ml-12",children:"@article{grndctrl2026,\n      title={GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment}, \n      author={[Authors to be added]},\n      journal={CVPR},\n      year={2026},\n      url={https://arxiv.org/abs/[arxiv-id]}, \n}"})]})})]})}},9459:function(e,r,t){"use strict";t.d(r,{cn:function(){return n}});var s=t(7381),i=t(4083);function n(){for(var e=arguments.length,r=Array(e),t=0;t<e;t++)r[t]=arguments[t];return(0,i.m6)((0,s.Z)(r))}}},function(e){e.O(0,[976,20,519,525,744],function(){return e(e.s=3002)}),_N_E=e.O()}]);